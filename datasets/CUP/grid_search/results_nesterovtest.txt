CUP:
1: {'loss': 1.8573568989259692, 'layers': 0, 'units': 23, 'learning_rate': 0.0025, 'batch_size': 32, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'None', 'regularization_lambda': 0, 'dropout': 0, 'epochs': 2000.0}2: {'loss': 1.940612271625156, 'layers': 0, 'units': 23, 'learning_rate': 0.0025, 'batch_size': 32, 'init_function': 'normalized_xavier', 'momentum': 0.5, 'regularizator': 'None', 'regularization_lambda': 0, 'dropout': 0, 'epochs': 2000.0}3: {'loss': 3.2661586221722967, 'layers': 0, 'units': 23, 'learning_rate': 0.0025, 'batch_size': 64, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'None', 'regularization_lambda': 0, 'dropout': 0, 'epochs': 2000.0}4: {'loss': 3.3751608452015796, 'layers': 0, 'units': 23, 'learning_rate': 0.0025, 'batch_size': 32, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 880.0}5: {'loss': 3.4578354059895045, 'layers': 0, 'units': 23, 'learning_rate': 0.0025, 'batch_size': 32, 'init_function': 'normalized_xavier', 'momentum': 0.5, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 914.0}6: {'loss': 3.4642681093389776, 'layers': 0, 'units': 23, 'learning_rate': 0.000625, 'batch_size': 32, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'None', 'regularization_lambda': 0, 'dropout': 0, 'epochs': 2000.0}7: {'loss': 3.5648086394460456, 'layers': 0, 'units': 23, 'learning_rate': 0.000625, 'batch_size': 32, 'init_function': 'normalized_xavier', 'momentum': 0.5, 'regularizator': 'None', 'regularization_lambda': 0, 'dropout': 0, 'epochs': 2000.0}8: {'loss': 3.771456538327359, 'layers': 0, 'units': 23, 'learning_rate': 0.0025, 'batch_size': 64, 'init_function': 'normalized_xavier', 'momentum': 0.5, 'regularizator': 'None', 'regularization_lambda': 0, 'dropout': 0, 'epochs': 2000.0}9: {'loss': 5.046916462424335, 'layers': 0, 'units': 23, 'learning_rate': 0.0025, 'batch_size': 64, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 1773.6666666666667}10: {'loss': 5.341637870913975, 'layers': 0, 'units': 23, 'learning_rate': 0.0025, 'batch_size': 64, 'init_function': 'normalized_xavier', 'momentum': 0.5, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 1319.6666666666667}