CUP:
1: {'loss': 3.362851286734595, 'layers': 1, 'units': 20, 'learning_rate': 0.0001, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 560}2: {'loss': 3.4450276490864447, 'layers': 2, 'units': 20, 'learning_rate': 0.0001, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 610}3: {'loss': 3.483364826799381, 'layers': 1, 'units': 15, 'learning_rate': 0.0001, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 624}4: {'loss': 3.5791541441890247, 'layers': 1, 'units': 20, 'learning_rate': 0.0001, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 1000}5: {'loss': 3.5979891867125517, 'layers': 2, 'units': 15, 'learning_rate': 0.0001, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 706}6: {'loss': 3.6876987629738993, 'layers': 2, 'units': 20, 'learning_rate': 0.0001, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 1000}7: {'loss': 3.6964993826652406, 'layers': 1, 'units': 10, 'learning_rate': 0.0001, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 524}8: {'loss': 3.7091532255678024, 'layers': 1, 'units': 15, 'learning_rate': 0.0001, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 1000}9: {'loss': 3.7383910822168787, 'layers': 2, 'units': 15, 'learning_rate': 0.0001, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 1000}10: {'loss': 4.00125051999701, 'layers': 2, 'units': 10, 'learning_rate': 0.0001, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 878}