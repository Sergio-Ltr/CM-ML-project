CUP:
1: {'loss': 1.102609736547487, 'layers': 0, 'units': 23, 'learning_rate': 0.0025, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 528}2: {'loss': 1.1193583761108215, 'layers': 0, 'units': 24, 'learning_rate': 0.0025, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 472}3: {'loss': 1.1201704659415805, 'layers': 0, 'units': 25, 'learning_rate': 0.0025, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 721}4: {'loss': 1.1265882378865522, 'layers': 0, 'units': 21, 'learning_rate': 0.0025, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 330}5: {'loss': 1.1280484388470313, 'layers': 0, 'units': 20, 'learning_rate': 0.0025, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 387}6: {'loss': 1.1296368585027432, 'layers': 0, 'units': 23, 'learning_rate': 0.0025, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 461}7: {'loss': 1.1470732723219002, 'layers': 0, 'units': 22, 'learning_rate': 0.0025, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 596}8: {'loss': 1.1490409964697468, 'layers': 0, 'units': 24, 'learning_rate': 0.0025, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 331}9: {'loss': 1.1506119195922313, 'layers': 0, 'units': 20, 'learning_rate': 0.0025, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 405}10: {'loss': 1.1714220354355807, 'layers': 0, 'units': 21, 'learning_rate': 0.0025, 'batch_size': 1, 'init_function': 'normalized_xavier', 'momentum': 0.8, 'regularizator': 'L2', 'regularization_lambda': 1e-05, 'dropout': 0, 'epochs': 370}